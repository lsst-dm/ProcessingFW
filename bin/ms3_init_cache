#!/usr/bin/env python
# $Id$
# $Rev::                                  $:  # Revision of last commit.
# $LastChangedBy::                        $:  # Author of last commit. 
# $LastChangedDate::                      $:  # Date of last commit.

""" Program to ingest data files that were created external to framework """

import tempfile
import argparse
import subprocess
import os
import re
import sys
from collections import OrderedDict
import wrappers.WrapperUtils as wraputils
import intgutils.wclutils as wclutils
import processingfw.pfwdb as pfwdb
from processingfw.pfwdefs import *
from processingfw.fwutils import *
import processingfw.pfwutils as pfwutils
import processingfw.errors
import filemgmt.cache as cache

VERSION = '$Rev$'

def parse_provided_list(listname):
    """ create dictionary of files from list in file """

    filelist = {}
    try:
        with open(listname, "r") as fh:
            for line in fh:
                (fullname, filetype, cachepath) = fwsplit(line, ',')
                (path, fname) = os.path.split(fullname)
                filelist[fname] = {'path': path, 'filetype': filetype, 'fullname':fullname, 'cachepath':cachepath}
    except Exception as err:
        fwdie("Problems reading file '%s': %s" % (listname, err), PF_EXIT_FAILURE)
        
    return filelist


def get_list_filenames(ingestpath, filetype):
    """ create a dictionary of files in given path """

    filelist = {}
    for f in os.listdir(ingestpath):
        if os.path.isfile(ingestpath+'/'+f):
            filelist[f]  = {'path': ingestpath, 'filetype': filetype}
    return filelist


def list_not_in_db(filenames, dbh):
    """ Return list of files from given set which are not in database """

    origcount = len(filenames)

    print "Checking if files are already in database"
    dbq = "select filename from genfile where filename in ('%s')" % "','".join(filenames)
    curs = dbh.cursor()
    curs.execute(dbq)
    dblist = []
    for row in curs:
        dblist.append(row[0])

    filenames = set(filenames) - set(dblist)
    print "\t%0d files already in database" % len(dblist)

    if len(filenames) > 0:
        print "\t%0d files still to be ingested" % len(filenames)

    return filenames


def get_metadata_specs(ftype, dbwcl):
    metaspecs = pfwutils.get_metadata_wcl(ftype, ftype, dbwcl)
    for key in metaspecs:
        if type(metaspecs[key]) == dict or type(metaspecs[key]) == OrderedDict:
            if 'wcl' in metaspecs[key]:
                del metaspecs[key]['wcl']    # remove wcl requirements for manual file ingestion
            if len(metaspecs[key]) == 0:
                del metaspecs[key]
    return metaspecs


def process_files(filelist, dbwcl, dbh, cachename, noingest=False):
    """ Ingests file metadata for all files in filelist """ 

    print "\nProcessing %0d files" % (len(filelist))
    if DATA_DEF not in dbwcl:
        fwdie("Error: %s not in database wcl" % DATA_DEF, PF_EXIT_FAILURE)

    if cachename not in dbwcl[DATA_DEF]:
        fwdie("Error: %s not in OPS_DATA_DEF" % cachename, PF_EXIT_FAILURE)

    cachedict = dbwcl[DATA_DEF][cachename]

    # group by filetype
    byfiletype = {}
    for f in filelist:
        filetype = filelist[f]['filetype']
        if filetype not in byfiletype:
            byfiletype[filetype] = {'fullname': [], 'cachefinfo': {}}
        fullname = filelist[f]['fullname']
        byfiletype[filetype]['fullname'].append(fullname)
        byfiletype[filetype]['cachefinfo'][fullname] = filelist[f]['cachepath']

    filedata = {}
    for ftype in byfiletype:
        print "%s:" % ftype
        metaspecs = get_metadata_specs(ftype, dbwcl['filetype_metadata'])
        metaspecs['filetype'] = ftype
        metaspecs['fullname'] = ','.join(sorted(byfiletype[ftype]['fullname']))
        print "\tGathering file metadata on %0d files...." % len(byfiletype[ftype]['fullname']), 
        #wclutils.write_wcl(metaspecs)
        filemeta = wraputils.get_file_metadata(metaspecs)
        #wclutils.write_wcl(filemeta)
    
        # add filenames and filetypes to metadata
        for fdict in filemeta.values():
            fdict['filename'] = wclutils.getFilename(fdict['fullname'])
            fdict['filetype'] = ftype

        print "DONE"

        if not noingest:
            print "\tCalling ingest_file_metadata on %s files..." % len(filemeta), 
            try:
                dbh.ingest_file_metadata(filemeta, dbwcl['filetype_metadata'])
            except processingfw.errors.FileMetadataIngestError as err:
                fwdie("\nERROR: %s" % err, PF_EXIT_FAILURE)

            print "DONE"

        #wclutils.write_wcl(cachedict)
        #wclutils.write_wcl(cachefileinfo)
        print "\tCalling filecache.put_within_job on %s files..." % len(byfiletype[ftype]['cachefinfo']),
        try:
            filecache = cache.Cache()
            problemfiles = filecache.put_within_job(byfiletype[ftype]['cachefinfo'], cachedict)
    
            if len(problemfiles) > 0:
                print "ERROR\nProblems putting %0d files into cache" % len(problemfiles)
                for file in problemfiles:
                    print file, "\n"
                sys.exit(PF_EXIT_FAILURE)
        except Exception as err:
            fwdie("ERROR\n%s" % err, PF_EXIT_FAILURE)

        print "DONE"

     
def main(args):
    parser = argparse.ArgumentParser(description='Ingest metadata for files generated outside DESDM framework')
    parser.add_argument('--dbwcl', action='store')
    parser.add_argument('--noingest', action='store_true', default=False)
    parser.add_argument('--list', action='store', help='format:  fullname, filetype, cachepath')
    parser.add_argument('--cachename', action='store', help='single value')
    parser.add_argument('--cachepath', action='store', help='single value, must also specify path and filetype')
    parser.add_argument('--filetype', action='store', help='single value, must also specify path and cachepath')
    parser.add_argument('--path', action='store', help='single value, must also specify filetype and cachepath')
    parser.add_argument('--version', action='store_true', default=False)

    args = vars(parser.parse_args())

    sys.stdout = os.fdopen(sys.stdout.fileno(), 'w', 0)
    #print 'Using version %s of file_metadata_ingest\n' % VERSION
    #
    #if args['version']:
        # return 0

    if not args['cachename']:
        print "Error: must specify cachename\n"
        parser.print_help()
        return 1

    if args['filetype'] and ',' in args['filetype']: 
        print "Error: filetype must be single value\n"
        parser.print_help()
        return 1

    if args['path'] and ',' in args['path']:
        print "Error: path must be single value\n"
        parser.print_help()
        return 1

    if args['filetype'] and args['path'] is None:
        print "Error: must specify path if using filetype\n" 
        parser.print_help()
        return 1

    if args['filetype'] is None and args['path']:
        print "Error: must specify filetype if using path\n" 
        parser.print_help()
        return 1

    if not args['filetype'] and not args['list']:
        print "Error: must specify either list or filetype+path\n"
        parser.print_help()
        return 1


    dbh = pfwdb.PFWDB()
    dbwcl = dbh.get_database_defaults()

    if args['dbwcl'] is not None:
        with open(args['dbwcl'], 'w') as fh:
            wclutils.write_wcl(dbwcl, fh, True, 4)

    if args['path'] is not None:
        filelist = get_list_filenames(args['path'], args['filetype'])
    elif args['list'] is not None:
        filelist = parse_provided_list(args['list'])

    print "Asked to ingest %0d files\n" % (len(filelist))
    filenames = list_not_in_db(filelist.keys(), dbh)

    if len(filenames) == 0:
        print "\nAll files already in database"
    else:
        # create input list of files not already in db
        insfilelist = {}
        for f in filenames:
            insfilelist[f] = filelist[f]

        process_files(insfilelist, dbwcl, dbh, args['cachename'], args['noingest'])
        dbh.commit()
    
if __name__ == '__main__':
    sys.exit(main(sys.argv))

